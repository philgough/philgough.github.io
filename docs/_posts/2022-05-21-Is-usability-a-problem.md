---
layout: post
title: Is usability a problem?
categories: [Design, Usability, Opinion, Rant]
date: 21/05/2022
---

{% newthought 'Is design generating problems under the guise of usability,' %} when what is should be doing is understanding and solving them?<!--more--> For example, I bought a coffee press online recently. It's terrible: the press filter doesn't quite seal, so the coffee is silty; there is a chunky plastic housing around the glass carafe which makes the whole thing too big to hold comfortably; lid is too chunky, so too much of my coffee won't pour out, so I tip it over further, and the angle changes too much so the spout fails to work properly, with the result that coffee ends up between the housing and the glass carafe inside. It's just a terrible, frustrating experience, especially as I experience it _before_ the coffee. My problem is that it's a product that I shouldn't even think about. My goal is not to experience the use of the object, or to think about it at all; it's for the object to let me experience the coffee-making _process_{% sidenote 'sn-id-coffee-plz' 'or, importantly, the coffee itself.' %} through a self-effacing design. 

Ok, _I_ should have made a better choice, but maybe I could warn someone else by leaving a review. I don't usually write reviews, but this time I thought I would do so. The website I purchased it from had a kind of virtual "recommended" sticker on the product (they haven't decided to take that down since I contacted them to complain), as well as good reviews, both of which, I feel, influenced my decision to make that purchase. The website invited me to review their company, unsurprisingly, but not the actual product. There's no way for me to leave a review on the actual product on their page, since the reviews are hosted by a third party, [Trustpilot](https://www.trustpilot.com){:target="_blank"}. This site only allows you to post a review if you are invited or have supplied this third party with some form of personal ID. 

How does this service increase agency for me as a user (to give a contrasting perspective to the existing reviews), or encourage trust in their service? Now that I realise that reviews are by invitation or people willing to hand over personally identfiying information, why should I trust any of those provided by that service (though they say that their mission is to combat fake reviews). I'm not suggesting that Trustpilot can't be trusted, and as far as I know they haven't had a security breach, but if exposing myself to risk of having personally identifiable information shared with a company I have no other contact with, I might just choose not to warn others about how disappointing a moderately expensive coffee maker is. 

I assume I'm not the only one who feels that way. 


## Victim of Usability
Of course, Trustpilot provides a service to online businesses, not the reviewers, who are the product. Trustpilot will manage the database of reviews instead of the retailer, and claim to maintain the integrity of the reviews, though, I feel that the process of adding a review reduces my confidence that they have a balanced view of anything. Trolls and fake reviews are an obvious problem, but to address that, there's now a risk for the reviewer if they have a data breach. Is it a fair expectation that I, the user of a online coffee retailer's website, would trust this entire network of actors to behave in my best interest?

An article by Coulton & Lindley{% sidenote 'sn-id-coulton-lindley' 'Coulton, P., & Lindley, J. G. (2019). More-Than Human Centred Design: Considering Other Things. The Design Journal, 22(4), 463â€“481. DOI: [10.1080/14606925.2019.1614320](https://doi.org/10.1080/14606925.2019.1614320){:target="_blank"}' %} describes the case of a smart TV that was selling data logged from users to a third party. This, of course, wasn't the problem{% sidenote 'sn-id-totally-the-problem' 'it _totally_ is a problem, but that is soooo not how the TV manufacturers see it...' %}---the actual problem was that they had no right to do so. It _should_ have been part of the TV's license agreement but wasn't. These agreements, as Coulton & Lindley suggest, are made so "usable" that the user ends up with diminished agency, being steered towards something they probably don't _really_ want to do: sell their data to third parties, without making [the process of opting-out](https://www.tomsguide.com/how-to/stop-your-snooping-smart-tv-how-to-turn-off-data-collection-for-every-brand){:target="_blank"} equally as "usable." As a researcher, my inclination is that this does not actually constitute _informed_ consent. 

This isn't just an academic discussion. Part of the point that Coulton & Lindley make is that the speed at which we expect interaction to happen is one source of issues for users. I have not unboxed a new TV that often, but in my limited experience, I was not excited about doing a deep-dive on EULAs, or even thinking about my privacy, because it's a TV, it _receives_ the signal, DUH! So my, and probably your, TV is selling viewing data{% sidenote 'sn-id-ads' 'because, of course, targeted ads are what I want to get out of it, right?' %} while at the same time sweeping the user through a consent process that they probably don't understand, or care to pay attention to. They manufacturer won't remind the user that the TV aggregates their data, and say at a more convenient time "do you still wanna do this, to let us sell this data collected using a TV you paid for?" Doesn't sound like a good idea when you say it like that though...

## But we do need Usability
I was asked to give a guest lecture to introduce basic principles of interaction design to a class of Biomedical Engineering students. I've given this lecture a few years running now and it's pretty easy to deliver, but I had other things on in the morning, and wanted a convenient dose of caffeine before the class. I went to one of the vending machines on campus that I have used before, tapped my card, and went to key in the number of the (sugar-free) soft drink{% sidenote 'sn-id-drink' 'soda, fizzy drink, etc.'  %} that I wanted. Before I could enter the number, down fell an mango-flavoured aloe drink. There didn't seem to be anyone else around, I hadn't lined up behind anyone, but the previous user's input was still stored in the interface. Who knows how long it has been since someone wanted to know the cost of that drink, but not purchased it?

In my defence, these vending machines are relatively new, and the modularised format means that the interface for paying is completely separate from the interface for selecting a beverage, rather than integrated. However, preferring speed of interaction over the system's confidence of what I want means that the error cost me what I wanted, and was a frustrating experience, something you never want in a user interface. It would be reasonable to have a confirmation if the product was selected before payment, and reduce the risk of this error, as a slip here takes my money away. 

### Can we just slow down a little?
Fortunately, as I was about to give a lecture about fundamentals of usability evaluations, it was a helpful analogy. The sugar crash{% sidenote 'sn-id-sugar' 'there is a LOT more sugar in those drinks than you may expect' %} wasn't as helpful as the caffeine boost I wanted, but the frustration at this stupid machine carried me through the lecture. 

The latest concerning technology trend I have noticed is cameras on self-serve supermarket checkout systems that capture the face of the customer as they use the system. I don't recall consenting to being recorded, though there's no assumed privacy in a supermarket, so consent isn't explicitly needed. Again, I do not trust these big companies to act in my best interest, as they already collect a lot of data about us through rewards programs, and probably other means. As there are examples of all kinds of data that are used about shoppers{% sidenote 'sn-id-target' 'such as described in this Forbes.com article: [How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did](https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/?sh=5f9717ee6668){:target="_blank"}' %}, and I'm comfortable assuming they are using the images for more than stopping shoplifting or "magically" speeding up the checkout process by having another camera on the food that's being scanned. How do I know if they're not also seeing whether I bring a reusable bag from Coles into Woolworths, and adding that to a profile they have on me, or whether they're reading a brand on my shirt or hat? Again, this pushes consequences (particularly lack of privacy and discomfort) onto the user, in order to deal with symptoms of problems (maybe shoplifting?), rather than addressing those problems themselves.

The supermarket chains don't seem to have any policy about the use of footage, based on a quick search{% sidenote 'sn-id-itshouldbeeasy' 'I maintain that it should not difficult to find.' %} [though they claim not to record the data](https://thenewdaily.com.au/life/tech/2020/06/15/supermarket-cameras-self-serve/){:target="_blank" %}. In contrast, I noticed a lot of CCTV cameras at a park in my local council area today while walking my dog. However, at these are governed by some kind of policy that is very simple to find online. I'm not saying I'm stoked about being filmed in a public park, but at least I can access information about how it is used and that someone has at least thought about it, and there is some respect for me as a human being. Supermarkets need some consideration of the actual user---I mean person---and their privacy, to make it clear why they being filmed, why and how it's stored or used. Additionally, It's reasonable to feel that a public park is more risky than a supermarket, especially, say, at night, so the CCTVs in that public space _may_ give some people a greater sense of confidence or safety. Or maybe not. However, targeted surveillance in a supermarket checkout seems to treat me like a criminal, rather than offer some feeling of protection. 

## Balancing Usability
Usability isn't a quick process, it takes time to do well, and there are is always a trade-off to make. It seems that in a lot of cases designers weren't able or willing to make a strong enough case for the value that comes from good usability. I can identify this, as I have had this challenge even in research projects. Unless people learn the benefit and importance of thinking about usability, they probably won't. However, in some cases it seems that Coulton & Lindley are right in suggesting that usability creates a screen to hide less-than-good intentions of corporations from the end-users (aka people).

This makes me question usability itself. Is it Good for usability to rush us through interactions that carry risk, even minor inconvenience? Is it really ethical for usability to be a mask for bad intentions? As designers, can the decision to hide away diminished rights to privacy and agency ever be justified in the name of the _ease of interaction_? And do designers even have a say in these processes? Anyone could tell a story about a usability failure, so it's not being applied well every time. 

The design industry is successful when it makes good designs. Newness is nice, but not essential---the iPod wasn't the first mp3 player, but it was a really good one. In design research, we are, by definition, focused on novelty. However, creating "capital-g" _Good_ design should be what we have in common, and might be something that industry could learn from us researchers (who of course have all the time in the world...). Perhaps it's time to slow both interfaces and the design process down, so that speed-focused usability don't act as a screen that prevents people from making informed decisions about the products they choose and the data they share, makes them feel stupid for having difficulty with seemingly simple tasks, or assumes that users are fine with handing over any or all information about themselves without question. 